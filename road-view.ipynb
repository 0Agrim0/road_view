{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from glob import glob\n","import time\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import Sequential\n","from keras.models import Model\n","from keras.layers import Conv2D, Input, MaxPool2D, Conv2DTranspose, concatenate, Lambda, BatchNormalization, Activation, LeakyReLU, ReLU\n","from keras.utils import img_to_array, load_img, plot_model\n","from keras.optimizers import Adam\n","from keras.initializers import RandomNormal"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["path = \"../input/deepglobe-road-extraction-dataset/train/\"\n","num_images = 1000\n","\n","combined_images = sorted(glob(path + \"*.jpg\"))[:num_images]\n","    \n","images = np.zeros(shape=(len(combined_images), 256, 256, 3))\n","\n","for idx, path in enumerate(combined_images):\n","    combined_image = tf.cast(img_to_array(load_img(path)), tf.float32)\n","    image = combined_image \n","    images[idx] = (tf.image.resize(image,(256,256)))/255"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["path = \"../input/deepglobe-road-extraction-dataset/train/\"\n","num_images = 1000\n","\n","combined_images = sorted(glob(path + \"*.png\"))[:num_images]\n","    \n","masks = np.zeros(shape=(len(combined_images), 256, 256, 3))\n","\n","for idx, path in enumerate(combined_images):\n","    combined_image = tf.cast(img_to_array(load_img(path)), tf.float32)\n","    mask = combined_image \n","    masks[idx] = (tf.image.resize(mask,(256,256)))/255"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(25,10))\n","for i in range(1,6):\n","    idx = np.random.randint(1,10)\n","    image, mask = images[idx], masks[idx]\n","    plt.subplot(2,5,i)\n","    plt.imshow(image)\n","    plt.title(str(i) + \" .Satellite image\")\n","    plt.axis(\"off\")\n","    \n","    plt.subplot(2,5,i + 5)\n","    plt.imshow(mask)\n","    plt.title(str(i) + \" .Map \")\n","    plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def downscale(num_filters):\n","    block = Sequential()\n","    block.add(Conv2D(num_filters, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False))\n","    block.add(LeakyReLU(alpha=0.2))\n","    block.add(BatchNormalization())\n","    return block\n","\n","def upscale(num_filters):\n","    block = Sequential()\n","    block.add(Conv2DTranspose(num_filters, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False))\n","    block.add(LeakyReLU(alpha=0.2))\n","    block.add(BatchNormalization())\n","    block.add(ReLU())\n","    return block\n","\n","def Generator():\n","    inputs = Input(shape=(256,256,3), name=\"InputLayer\")\n","\n","    encoder = [\n","        downscale(64),\n","        downscale(128),\n","        downscale(256),\n","        downscale(512),\n","        downscale(1024),\n","        downscale(1024),\n","    ]\n","    \n","    latent_space = downscale(1024)\n","\n","    decoder = [\n","        upscale(1024),\n","        upscale(1024),\n","        upscale(512),\n","        upscale(256),\n","        upscale(128),\n","        upscale(64),\n","    ]\n","    \n","    x = inputs \n","    skips = []\n","    for layer in encoder:\n","        x = layer(x)\n","        skips.append(x)\n","    \n","    x = latent_space(x)\n","\n","    skips = reversed(skips)\n","    for up, skip in zip(decoder, skips):\n","        x = up(x)\n","        x = concatenate([x, skip])\n","    \n","    initializer = RandomNormal(stddev=0.02, seed=42)\n","    outputs = Conv2DTranspose(3, kernel_size=4, strides=2, kernel_initializer = initializer, activation = 'tanh', padding = 'same')\n","\n","    outputs = outputs(x)\n","\n","    generator = Model(inputs = inputs, outputs = outputs, name=\"Generator\")\n","    return generator"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generator = Generator()\n","plot_model(generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def Discriminator():\n","    image = Input(shape = (256,256,3), name = \"ImageInput\")\n","    target = Input(shape = (256,256,3), name = \"TargetInput\")\n","    x = concatenate([image, target])\n","\n","    x = downscale(64)(x)\n","    x = downscale(128)(x)\n","    x = downscale(512)(x)\n","\n","    initializer = RandomNormal(stddev = 0.02, seed=42)\n","        \n","    x = Conv2D(512, kernel_size = 4, strides = 1, kernel_initializer = initializer, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","\n","    x = Conv2D(1, kernel_size = 4, kernel_initializer = initializer)(x)\n","\n","    discriminator = Model(inputs = [image, target], outputs = x, name = \"Discriminator\")\n","\n","    return discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["discriminator = Discriminator()\n","plot_model(discriminator)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["adversarial_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","def dice_loss(target, predicted):\n","    smooth = 1e-6\n","    intersection = tf.reduce_sum(target * predicted)\n","    union = tf.reduce_sum(target) + tf.reduce_sum(predicted)\n","    dice = (2.0 * intersection + smooth) / (union + smooth)\n","    return 1.0 - dice\n","\n","def generator_loss(discriminator_generated, generated_output, target_image):\n","    gan_loss = adversarial_loss(tf.ones_like(discriminator_generated), discriminator_generated)\n","    dice_loss_val = dice_loss(target_image, generated_output)\n","    total_loss = (100 * dice_loss_val) + gan_loss\n","    return total_loss, gan_loss, dice_loss_val\n","\n","def discriminator_loss(discriminator_real_output, discriminator_generated_output):\n","    real_loss = adversarial_loss(tf.ones_like(discriminator_real_output), discriminator_real_output)\n","    fake_loss = adversarial_loss(tf.zeros_like(discriminator_generated_output), discriminator_generated_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","def train_step(inputs, target):\n","    with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n","        generated_output = generator(inputs, training=True)\n","        \n","        discriminator_real_output = discriminator([inputs, target], training=True)\n","        discriminator_generated_output = discriminator([inputs, generated_output], training=True)\n","        \n","        generator_total_loss, generator_gan_loss, generator_l1_loss = generator_loss(discriminator_generated_output, generated_output, target)\n","        \n","        discriminator_Loss = discriminator_loss(discriminator_real_output, discriminator_generated_output)\n","        \n","    generator_gradients = generator_tape.gradient(generator_total_loss, generator.trainable_variables)\n","    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n","     \n","    discriminator_gradients = discriminator_tape.gradient(discriminator_Loss, discriminator.trainable_variables)\n","    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def fit(data, epochs, verbose=1):\n","    for epoch in range(epochs):\n","        start = time.time()\n","        if verbose >= 1:\n","            print(\"Current epoch: \", epoch+1)\n","        for image, mask in data:\n","            train_step(image, mask)\n","        if verbose >= 1:\n","            print(f\"Time taken to complete the epoch {epoch + 1} is {(time.time() - start):.2f} seconds \\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sat_image, map_image = tf.cast(images, tf.float32), tf.cast(masks, tf.float32)\n","dataset = (sat_image, map_image)\n","data = tf.data.Dataset.from_tensor_slices(dataset).batch(32, drop_remainder=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fit(data, 40, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def show_predictions(num_samples):\n","    for i in range(num_samples):\n","        idx = np.random.randint(images.shape[0])\n","        image, mask = images[idx], masks[idx]\n","        predicted = generator.predict(tf.expand_dims(image, axis=0))[0]\n","        \n","        plt.figure(figsize=(10,8))\n","        \n","        plt.subplot(1,3,1)\n","        plt.imshow(image)\n","        plt.title(\"Satellite Image \" + str(idx))\n","        plt.axis('off')\n","        \n","        plt.subplot(1,3,2)\n","        plt.imshow(mask)\n","        plt.title(\"Map Image \" + str(idx))\n","        plt.axis('off')\n","        plt.subplot(1,3,3)\n","        plt.imshow(predicted)\n","        plt.title(\"Predicted Image \" + str(idx))\n","        plt.axis('off')\n","        \n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["show_predictions(5)\n","image, mask = images[621], masks[621]\n","predicted = generator.predict(tf.expand_dims(image, axis=0))[0]\n","        \n","plt.figure(figsize=(10,8))\n","        \n","plt.subplot(1,3,1)\n","plt.imshow(image)\n","plt.title(\"Satellite Image \" + \"This\")\n","plt.axis('off')\n","        \n","plt.subplot(1,3,2)\n","plt.imshow(mask)\n","plt.title(\"Map Image \" + \"This\")\n","plt.axis('off')\n","        \n","plt.subplot(1,3,3)\n","plt.imshow(predicted)\n","plt.title(\"Predicted Image \" + \"This\")\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import math\n","\n","def calculate_metrics(images, masks, generator, num_samples):\n","    mse_scores = []\n","    mae_scores = []\n","\n","    for i in range(num_samples):\n","        idx = np.random.randint(images.shape[0])\n","        image, mask = images[idx], masks[idx]\n","        predicted = generator.predict(np.expand_dims(image, axis=0))[0]\n","        \n","        # Calculate MSE\n","        mse_score = np.mean((mask - predicted) ** 2)\n","        mse_scores.append(mse_score)\n","        \n","        # Calculate MAE\n","        mae_score = np.mean(np.abs(mask - predicted))\n","        mae_scores.append(mae_score)\n","        avg_mse = np.mean(mse_scores)\n","        avg_mae = np.mean(mae_scores)\n","    \n","    return avg_mse, avg_mae\n","\n","# Usage\n","num_samples = 100  # Number of samples to evaluate\n","avg_mse, avg_mae = calculate_metrics(images, masks, generator, num_samples)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Average MSE:\", avg_mse * 100)\n","print(\"Average MAE:\", avg_mae * 100)\n","print(\"Average RMSE:\",math.sqrt(avg_mse) * 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import math\n","\n","def calculate_metrics(images, masks, generator, num_samples):\n","    mse_scores = []\n","    mae_scores = []\n","\n","    for i in range(num_samples):\n","        idx = np.random.randint(images.shape[0])\n","        image, mask = images[idx], masks[idx]\n","        predicted = generator.predict(np.expand_dims(image, axis=0))[0]\n","        \n","        # Calculate MSE\n","        mse_score = np.mean((mask - predicted) ** 2)\n","        mse_scores.append(mse_score)\n","        \n","        # Calculate MAE\n","        mae_score = np.mean(np.abs(mask - predicted))\n","        mae_scores.append(mae_score)\n","        avg_mse = np.mean(mse_scores)\n","    avg_mae = np.mean(mae_scores)\n","    \n","    return avg_mse, avg_mae\n","\n","# Usage\n","num_samples = 100  # Number of samples to evaluate\n","avg_mse, avg_mae = calculate_metrics(images, masks, generator, num_samples)\n","print(\"Average MSE:\", avg_mse * 100)\n","print(\"Average MAE:\", avg_mae * 100)\n","print(\"Average RMSE:\",math.sqrt(avg_mse) * 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generator.save(\"GAN_Sat_image_2_map.h5\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":966140,"sourceId":1634186,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
